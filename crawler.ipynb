{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "\n",
    "The script crawls articles on Feedspot's blog to gather RSS feeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Loads the following data from the database and stores it in global variables:\n",
    "* Feed urls\n",
    "* Unvisited lists (articles) urls\n",
    "* Visited lists (articles) urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# first list url to be scraped if there is not data in db\n",
    "firstListUrl = 'https://blog.feedspot.com/uk_rss_feeds/'\n",
    "\n",
    "# list of all feeds\n",
    "feeds = []\n",
    "\n",
    "unvisitedListUrls = []\n",
    "visitedListUrls = []\n",
    "\n",
    "# initial lengths of lists to make sure\n",
    "# only new data is added to the db\n",
    "initialNoFeeds = 0\n",
    "initialNoVisitedLists = 0\n",
    "\n",
    "# number n of visited lists\n",
    "# will be used to remove top n urls from db\n",
    "visitedLists = 0\n",
    "\n",
    "# load data from the database\n",
    "def loadData():\n",
    "\n",
    "    print('Loading data from db...')\n",
    "\n",
    "    # connect to the database\n",
    "    conn = sqlite3.connect('feeds_dev.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # load feeds\n",
    "    c.execute('SELECT url FROM feeds;')\n",
    "    global feeds\n",
    "    feeds = [i[0] for i in c.fetchall()]\n",
    "\n",
    "    global initialNoFeeds\n",
    "    initialNoFeeds = len(feeds)\n",
    "    print('Loaded ' + str(initialNoFeeds) + ' feeds')\n",
    "\n",
    "    # load unvisited lists\n",
    "    c.execute('SELECT url FROM unvisited_lists;')\n",
    "    global unvisitedListUrls\n",
    "    unvisitedListUrls = [i[0] for i in c.fetchall()]\n",
    "\n",
    "    global initialNoUnvisitedLists\n",
    "    initialNoUnvisitedLists = len(unvisitedListUrls)\n",
    "    print('Loaded ' + str(initialNoUnvisitedLists) + ' unvisited lists')\n",
    "    \n",
    "    # add an url if the db is empty\n",
    "    if (len(unvisitedListUrls) == 0):\n",
    "        unvisitedListUrls.append(firstListUrl)\n",
    "\n",
    "    # load visited lists\n",
    "    c.execute('SELECT url FROM visited_lists;')\n",
    "    global visitedListUrls\n",
    "    visitedListUrls = [i[0] for i in c.fetchall()]\n",
    "\n",
    "    global initialNoVisitedLists\n",
    "    initialNoVisitedLists = len(visitedListUrls)\n",
    "    print('Loaded ' + str(initialNoVisitedLists) + ' visited lists')\n",
    "    print('\\n')\n",
    "\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape List of Feeds\n",
    "\n",
    "Define a function that scrapes the url of a given Feedspot article that contains a list of RSS feeds, as well as a list of links to similar articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves all rss feed links from the given url\n",
    "# and adds them to the global variable\n",
    "def scrapeUrl(url):\n",
    "\n",
    "    # retrieve the webpage content\n",
    "    # include user-agent to ensure the response is not 403 Forbidden\n",
    "    try: \n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Mobile Safari/537.36'}\n",
    "        webpage_response = requests.get(url, headers=headers)\n",
    "\n",
    "        # get the rss feeds on that webpage\n",
    "        soup = BeautifulSoup(webpage_response.content, 'html.parser')\n",
    "        for tag in soup.select('.trow .fa-rss + a'):\n",
    "\n",
    "            href = tag.attrs['href']\n",
    "            if href != '' and href not in feeds:\n",
    "                feeds.append(tag.attrs['href'])\n",
    "\n",
    "        # get the links to the feed lists on the page\n",
    "        for tag in soup.select('.et_pb_extra_column_sidebar a'):\n",
    "\n",
    "            # href only contains the path, construct complete url\n",
    "            listUrl = 'https://blog.feedspot.com' + tag.attrs['href']\n",
    "            \n",
    "            # if the list has not been visited before,\n",
    "            # add it to the unvisited lists\n",
    "            \n",
    "            # there may be lists of other content types (blog, website etc.) \n",
    "            # only add rss feed lists\n",
    "            if listUrl not in visitedListUrls and listUrl not in unvisitedListUrls and 'rss_feeds' in listUrl:\n",
    "                unvisitedListUrls.append(listUrl)\n",
    "    except:\n",
    "        print('Could not scrape url: ' + url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl\n",
    "\n",
    "Define a function that scrapes the contents of the initial url provided and then subsequently scrapes each unvisited url (stored in a global list).\n",
    "\n",
    "The crawler stops when there are no more unvisited urls or when a predefined number of feeds (passed as an argument to the function) has been scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide a maximum number of feeds to be collected\n",
    "def crawl(maxNoOfFeeds):\n",
    "\n",
    "    # scrape webpages until there are no more lists to be scraped\n",
    "    # or the maximum number of feeds has been exceeded\n",
    "    while unvisitedListUrls and len(feeds) - initialNoFeeds < maxNoOfFeeds:\n",
    "\n",
    "        listUrl = unvisitedListUrls.pop(0)\n",
    "\n",
    "        # increment the number of visited lists in this run of the crawler\n",
    "        global visitedLists\n",
    "        visitedLists += 1\n",
    "\n",
    "        # add the list URL to the visited URLs\n",
    "        visitedListUrls.append(listUrl)\n",
    "\n",
    "        print('Scraping list: ', listUrl)\n",
    "        scrapeUrl(listUrl)\n",
    "        print('Number of feeds: ', len(feeds))\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Database\n",
    "\n",
    "Delete all tables from the database and recreate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the sqlite3 database of feeds\n",
    "def resetDatabase():\n",
    "    \n",
    "    # connect to the database\n",
    "    conn = sqlite3.connect('feeds_dev.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # drop the tables\n",
    "    print('Deleting tables...')\n",
    "    c.execute('DROP TABLE IF EXISTS feeds;')\n",
    "    c.execute('DROP TABLE IF EXISTS posts;')\n",
    "    c.execute('DROP TABLE IF EXISTS unvisited_lists;')\n",
    "    c.execute('DROP TABLE IF EXISTS visited_lists')\n",
    "\n",
    "    # re-create the tables\n",
    "    print('Creating tables... \\n')\n",
    "    c.execute('CREATE TABLE feeds (_id INTEGER PRIMARY KEY, url TEXT, text TEXT, title TEXT, description TEXT);')\n",
    "    c.execute('CREATE TABLE posts (_id INTEGER PRIMARY KEY, title TEXT, description TEXT, text TEXT, feed_title TEXT, FOREIGN KEY (feed_title) REFERENCES feeds (title);')\n",
    "    c.execute('CREATE TABLE unvisited_lists (_id INTEGER PRIMARY KEY, url TEXT);')\n",
    "    c.execute('CREATE TABLE visited_lists (_id INTEGER PRIMARY KEY, url TEXT);')    \n",
    "\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data\n",
    "\n",
    "Define a function that saves all data that has been gathered by the crawler in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves all feeds from the global list into the sqlite3 database\n",
    "def saveData():\n",
    "\n",
    "    print('Saving feeds...')\n",
    "\n",
    "    # connect to the database\n",
    "    conn = sqlite3.connect('feeds_dev.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # save feeds\n",
    "    counter = 0\n",
    "    for feed in feeds[initialNoFeeds:]:\n",
    "        c.execute('INSERT INTO feeds (url) VALUES (?);', (feed,))\n",
    "        counter += 1\n",
    "\n",
    "    # update the unvisited lists in the database\n",
    "    c.execute('DELETE FROM unvisited_lists;')\n",
    "    for l in unvisitedListUrls:\n",
    "        c.execute('INSERT INTO unvisited_lists (url) VALUES (?);', (l,))\n",
    " \n",
    "    # save visited lists\n",
    "    for l in visitedListUrls[initialNoVisitedLists:]:\n",
    "        c.execute('INSERT INTO visited_lists (url) VALUES (?);', (l, ))\n",
    "\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "    print('Saved ' + str(counter) + ' feeds')\n",
    "    conn.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Crawler\n",
    "\n",
    "Crawl only feed urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from db...\n",
      "Loaded 13168 feeds\n",
      "Loaded 430 unvisited lists\n",
      "Loaded 311 visited lists\n",
      "\n",
      "\n",
      "Scraping list:  https://blog.feedspot.com/australian_beauty_rss_feeds/\n",
      "Number of feeds:  13241\n",
      "\n",
      "\n",
      "Scraping list:  https://blog.feedspot.com/afl_rss_feeds/\n",
      "Number of feeds:  13265\n",
      "\n",
      "\n",
      "Scraping list:  https://blog.feedspot.com/australian_photography_rss_feeds/\n",
      "Number of feeds:  13337\n",
      "\n",
      "\n",
      "Saving feeds...\n",
      "Saved 169 feeds\n"
     ]
    }
   ],
   "source": [
    "loadData()\n",
    "crawl(100)\n",
    "saveData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Info Scraper\n",
    "\n",
    "Gather information about feed urls saved in the database.\n",
    "\n",
    "Extract the following information:\n",
    "* Title\n",
    "* Description\n",
    "* Text\n",
    "\n",
    "*Text* is formed by concatenating the title and description of the feed with the title and description of each of its entries. *Text* must contain between 100 and 500 words and must not contain any HTML tags. All other preprocessing and feature extraction steps will be carried out in other parts of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Feed URLs\n",
    "\n",
    "Loads the urls of the feeds that have not been processed. Randomly sample to get a subset of feed urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import requests\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "count = 0\n",
    "\n",
    "# list of feed urls loaded from the db\n",
    "feed_url_list = []\n",
    "\n",
    "# dict of feeds\n",
    "# urls are keys the values are dicts containing\n",
    "# features, title and description for that feed\n",
    "feed_info = dict()\n",
    "\n",
    "# load feeds urls of feeds that do not have features yet\n",
    "# randomly sample the set of feeds\n",
    "def loadFeedUrls(no_feeds):\n",
    "\n",
    "    # clear feed urls list and feeds dict\n",
    "    global feed_url_list\n",
    "    feed_url_list.clear()\n",
    "    feed_info.clear()\n",
    "\n",
    "    global count\n",
    "    count = 0\n",
    "\n",
    "    print('Loading feed urls...')\n",
    "\n",
    "    # connect to the db\n",
    "    conn = sqlite3.connect('feeds.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # select all urls that have not been processed\n",
    "    c.execute('SELECT url FROM feeds WHERE text IS NULL OR title IS NULL OR description IS NULL;')    \n",
    "    for entry in c.fetchall():\n",
    "        feed_url_list.append(entry[0])\n",
    "\n",
    "    print('Loaded ' + str(len(feed_url_list)) + ' feeds')\n",
    "    feed_url_list = random.sample(feed_url_list, no_feeds)\n",
    "\n",
    "    print('Sampled ' + str(len(feed_url_list)) + ' feeds\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse a Feed\n",
    "\n",
    "Define a function that parses a feed given its url as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze a feed and generate its initial body of text\n",
    "def parseFeed(url):\n",
    "\n",
    "    global count\n",
    "    count += 1\n",
    "    print('Generating features for ', url, ' ', str(count))\n",
    "\n",
    "    # return if the feed can not be parsed\n",
    "    try:\n",
    "\n",
    "        # get the rss feed content from the url\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Mobile Safari/537.36'}\n",
    "        webpage = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "        d = feedparser.parse(webpage.content)\n",
    "    except Exception as e:\n",
    "        print('Could not parse feed ', url)\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    # body of text\n",
    "    raw_text = ''\n",
    "\n",
    "    # check that the feed has a title, description and at least one entry\n",
    "    title = d['feed'].get('title')\n",
    "    description = d['feed'].get('description')\n",
    "    entries = d['entries']\n",
    "\n",
    "    if not title or not description or len(entries) == 0:\n",
    "        print('Invalid feed')\n",
    "        \n",
    "        # feed is invalid\n",
    "        return\n",
    "\n",
    "    # feed is valid, continue feature extraction\n",
    "    # add title and description to body of text\n",
    "    raw_text = title + ' ' + description\n",
    "\n",
    "    # add the title and description of each entry to the body of text\n",
    "    for entry in entries:\n",
    "        \n",
    "        # get entry info\n",
    "        entry_title = entry.get('title')\n",
    "        entry_title = entry_title if entry_title is not None else ''\n",
    "\n",
    "        entry_description = entry.get('description')\n",
    "        entry_description = entry_description if entry_description is not None else ''\n",
    "\n",
    "        # add entry info to body of text\n",
    "        raw_text = raw_text + ' ' + entry_title + ' ' + entry_description\n",
    "\n",
    "    # remove html tags\n",
    "    raw_text = BeautifulSoup(raw_text, 'html.parser').get_text()\n",
    "    \n",
    "    # check that the text has at least 100 words\n",
    "    if len(raw_text.split()) > 100:\n",
    "\n",
    "        # select at most 500 words\n",
    "        raw_text = ' '.join(raw_text.split()[:500])\n",
    "\n",
    "        # add the raw text to the feed's dict entry\n",
    "        feed_info[url] = {\n",
    "            'title': title,\n",
    "            'description': description,\n",
    "            'text': raw_text\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Info\n",
    "\n",
    "Collect the required information for a specified number of feeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feed urls...\n",
      "Loaded 59916 feeds\n",
      "Sampled 100 feeds\n",
      "\n",
      "Generating features for  http://koalasplayground.com/feed/   1\n",
      "Could not parse feed  http://koalasplayground.com/feed/\n",
      "('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Generating features for  https://weddinginclude.com/category/wedding-cake/feed/   2\n",
      "Invalid feed\n",
      "Generating features for  https://data.fineartstudioonline.com/rssfeed.asp?id=24209   3\n",
      "Invalid feed\n",
      "Generating features for  https://www.personalfinanceplan.in/category/mutual-funds/feed/   4\n",
      "Invalid feed\n",
      "Generating features for  https://sqlundercover.com/feed/   5\n",
      "Generating features for  https://mocbuilder.com/feed/   6\n",
      "Generating features for  http://fysurf.com/feed/   7\n",
      "Generating features for  https://www.sparklybelly.com/feed/   8\n",
      "Generating features for  https://www.youtube.com/feeds/videos.xml?user=mngeocaching   9\n",
      "Invalid feed\n",
      "Generating features for  http://popdramatic.blogspot.in//feeds/posts/default   10\n",
      "Generating features for  https://www.xwordinfo.com/feed   11\n",
      "Generating features for  https://www.proverbialhomemaker.com/feed   12\n",
      "Generating features for  https://parentingnow.org/feed/   13\n",
      "Invalid feed\n",
      "Generating features for  https://succulentmarket.com/blogs/care.atom   14\n",
      "Invalid feed\n",
      "Generating features for  https://www.youtube.com/feeds/videos.xml?channel_id=UCFeUyPY6W8qX8w2o6oSiRmw   15\n",
      "Invalid feed\n",
      "Generating features for  http://petermooreblog.blogspot.com/feeds/posts/default?alt=rss   16\n",
      "Invalid feed\n",
      "Generating features for  https://www.mtn-world.com/en/feed/   17\n",
      "Generating features for  http://www.operatoday.com/index.xml   18\n",
      "Invalid feed\n",
      "Generating features for  http://www.thefashionamy.com/feeds/posts/default?alt=rss   19\n",
      "Generating features for  https://www.roblevy.com/Blog/Rss   20\n",
      "Generating features for  http://www.oafe.net/blog/feed   21\n",
      "Generating features for  https://www.modestmoney.com/feed   22\n",
      "Generating features for  https://www.indiva.com/blog/feed/   23\n",
      "Generating features for  http://copd.ie/links/news/feed/   24\n",
      "Invalid feed\n",
      "Generating features for  https://www.interactone.com/feed/   25\n",
      "Generating features for  https://mollyelkindtalkingtextiles.blogspot.com/feeds/posts/default?alt=rss   26\n",
      "Generating features for  https://weareneveralone.co/feed/   27\n",
      "Generating features for  https://www.roamingaroundtheworld.com/feed/   28\n",
      "Generating features for  https://www.financebrokerage.com/feed/   29\n",
      "Generating features for  https://www.raphatherapyservices.com/feed/   30\n",
      "Generating features for  https://stantatkinblog.wordpress.com/feed/   31\n",
      "Generating features for  https://mobilefirstcloudfirst.net/feed/   32\n",
      "Generating features for  http://feeds.feedburner.com/blogspot/PMBlE   33\n",
      "Generating features for  https://430box.com/feed/   34\n",
      "Generating features for  http://happygreylucky.com/feed/   35\n",
      "Generating features for  http://thefashionguitar.com/feed/   36\n",
      "Generating features for  https://www.ewash.com.au//feed/rss2   37\n",
      "Invalid feed\n",
      "Generating features for  http://www.cakewrecks.com/home?format=rss   38\n",
      "Invalid feed\n",
      "Generating features for  https://www.arthurcharlesdesign.com/new-blog?format=RSS   39\n",
      "Invalid feed\n",
      "Generating features for  https://lifebalancecoach.com.au/feed/   40\n",
      "Generating features for  https://feeds.feedblitz.com/my_weblog   41\n",
      "Invalid feed\n",
      "Generating features for  https://thehungryteacherblog.com/feed   42\n",
      "Generating features for  https://cityfarmer.info/feed/   43\n",
      "Generating features for  https://bimarc.co/feed/   44\n",
      "Generating features for  https://festination.com/magazine/rss/?x=1   45\n",
      "Invalid feed\n",
      "Generating features for  https://www.captaincharley.net/feed/   46\n",
      "Invalid feed\n",
      "Generating features for  https://socgen.ucla.edu/feed/   47\n",
      "Invalid feed\n",
      "Generating features for  https://topinfographic.com/feed/   48\n",
      "Could not parse feed  https://topinfographic.com/feed/\n",
      "HTTPSConnectionPool(host='topinfographic.com', port=443): Max retries exceeded with url: /feed/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000203D475A7C0>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed'))\n",
      "Generating features for  https://theruggedbros.com/blogs/news.atom   49\n",
      "Invalid feed\n",
      "Generating features for  https://www.chetanbro.com/feeds/posts/default   50\n",
      "Generating features for  https://sugarthumb.co.uk/feed/   51\n",
      "Invalid feed\n",
      "Generating features for  https://buildfire.com/feed   52\n",
      "Generating features for  http://rianaolckers.blogspot.com/feeds/posts/default?alt=rss   53\n",
      "Generating features for  https://jordansflooringoutlet.ca/blogs/news.atom   54\n",
      "Invalid feed\n",
      "Generating features for  https://bimchapters.blogspot.com/feeds/posts/default?alt=rss   55\n",
      "Generating features for  http://markmanson.net/feed   56\n",
      "Generating features for  http://feeds.feedburner.com/AMERICAblogGay   57\n",
      "Generating features for  https://www.waathletics.org.au/News/rss/3630/   58\n",
      "Could not parse feed  https://www.waathletics.org.au/News/rss/3630/\n",
      "HTTPSConnectionPool(host='www.waathletics.org.au', port=443): Max retries exceeded with url: /News/rss/3630/ (Caused by SSLError(SSLCertVerificationError(\"hostname 'www.waathletics.org.au' doesn't match 'catchall-server-others.s1111.sureserver.com'\")))\n",
      "Generating features for  https://www.tildasworld.com/category/patchwork-and-quilt/feed/   59\n",
      "Generating features for  https://whatkatewore.com/feed/   60\n",
      "Generating features for  https://news4masses.com/feed/   61\n",
      "Generating features for  https://eursap.eu/feed/   62\n",
      "Generating features for  https://www.youtube.com/feeds/videos.xml?channel_id=UCY_cwzSl87thoBBdPvlM4Gw   63\n",
      "Invalid feed\n",
      "Generating features for  http://blog.dancedirect.com/feed/   64\n",
      "Invalid feed\n",
      "Generating features for  https://inspontaneousspeech.com/feed/   65\n",
      "Generating features for  https://www.christianitycove.com/feed   66\n",
      "Generating features for  https://smestreet.in/feed/   67\n",
      "Generating features for  https://www.thebatflip26.com/blog-feed.xml   68\n",
      "Generating features for  http://scottlilly.com/feed/   69\n",
      "Generating features for  http://kevinrandle.blogspot.com/feeds/posts/default   70\n",
      "Generating features for  http://www.menswearstyle.co.uk/rss   71\n",
      "Generating features for  https://blog.volody.com/feed/   72\n",
      "Invalid feed\n",
      "Generating features for  http://www.ajrathbun.com/blog/feed/   73\n",
      "Invalid feed\n",
      "Generating features for  https://www.dogshaming.com/feed/   74\n",
      "Invalid feed\n",
      "Generating features for  https://freeinfographicssubmit.wordpress.com/feed/   75\n",
      "Generating features for  http://identify3d.com/blog/feed/   76\n",
      "Generating features for  https://nomadisbeautiful.com/feed/   77\n",
      "Generating features for  https://somejokeshere.blogspot.com/feeds/posts/default?alt=rss   78\n",
      "Generating features for  https://vancouversun.com/category/homes/gardening/feed   79\n",
      "Invalid feed\n",
      "Generating features for  https://thesongfoundry.com/feed/   80\n",
      "Generating features for  https://www.serenityspringsrecovery.com/feed/   81\n",
      "Invalid feed\n",
      "Generating features for  https://www.thetexasattorney.com/feed/   82\n",
      "Invalid feed\n",
      "Generating features for  https://parisleaf.com/feed/   83\n",
      "Invalid feed\n",
      "Generating features for  http://www.bpnews.net/rss   84\n",
      "Invalid feed\n",
      "Generating features for  https://www.laborandemploymentlawcounsel.com/feed/   85\n",
      "Generating features for  http://feeds.feedburner.com/bucketlistjourney/ibHU   86\n",
      "Invalid feed\n",
      "Generating features for  https://www.akeelvalentine.com/feed/   87\n",
      "Generating features for  https://nydivorcefirm.com/blog/feed/   88\n",
      "Invalid feed\n",
      "Generating features for  https://www.hello-adience.com/feed   89\n",
      "Generating features for  https://nobuckingway.com/feed/   90\n",
      "Invalid feed\n",
      "Generating features for  http://feeds.feedburner.com/BroncosGab   91\n",
      "Generating features for  https://www.juanafc.com/feed/   92\n",
      "Could not parse feed  https://www.juanafc.com/feed/\n",
      "HTTPSConnectionPool(host='www.juanafc.com', port=443): Max retries exceeded with url: /feed/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1123)')))\n",
      "Generating features for  https://eggdonor.com/feed/   93\n",
      "Generating features for  https://prod-qt-images.s3.amazonaws.com/production/thequint/feed.xml   94\n",
      "Invalid feed\n",
      "Generating features for  https://neecysnecessities.wordpress.com/feed/   95\n",
      "Generating features for  https://www.fantasybookreview.co.uk/blog/feed/   96\n",
      "Generating features for  https://textilefocus.com/feed/   97\n",
      "Invalid feed\n",
      "Generating features for  https://insurancetailors.co.uk/feed/   98\n",
      "Generating features for  https://www.missguided.co.uk/babezine/feed   99\n",
      "Generating features for  https://snookerhq.com/feed/   100\n",
      "\n",
      "Features generated for 100 urls\n",
      "\n",
      "Saving features...\n",
      "Saved feed information\n",
      "\n",
      "Loading feed urls...\n",
      "Loaded 59855 feeds\n",
      "Sampled 100 feeds\n",
      "\n",
      "Generating features for  https://iconlogic.blogs.com/weblog/rss.xml   1\n",
      "Generating features for  https://astralharmony.com/blog/feed/   2\n",
      "Generating features for  https://www.iriselmjewelry.com/handmade-jewelry-blog?format=RSS   3\n",
      "Generating features for  http://www.perthfoodreviews.com/feed   4\n",
      "Invalid feed\n",
      "Generating features for  https://catercurator.com/blog/feed/   5\n",
      "Generating features for  https://cdn.vortala.com/gen/blogs-3357-feed.xml   6\n",
      "Invalid feed\n",
      "Generating features for  https://www.careerinstem.com/blog/feed/   7\n",
      "Generating features for  https://www.framesofnature.com/feed/   8\n",
      "Generating features for  https://www.missyqiqi.com/feed/   9\n",
      "Generating features for  https://www.vam.ac.uk/blog/feed   10\n",
      "Generating features for  https://feedpress.me/thymeforcooking   11\n",
      "Generating features for  http://liveboldandbloom.com/feed   12\n",
      "Invalid feed\n",
      "Generating features for  https://www.mesotheliomaveterans.org/feed   13\n",
      "Generating features for  https://www.hatchbuck.com/blog/category/marketing-automation/feed/   14\n",
      "Generating features for  http://www.businessnews.com.au/rssfeed/latest.rss   15\n",
      "Generating features for  https://www.kokoroseboutique.com/blogs/online-fashion-trends.atom   16\n",
      "Invalid feed\n",
      "Generating features for  https://simplified-it-outsourcing.com/blog/feed   17\n",
      "Invalid feed\n",
      "Generating features for  https://bigfundraisingideas.com/blog/feed   18\n",
      "Invalid feed\n",
      "Generating features for  https://shiatsu-trish.blogspot.com/feeds/posts/default?alt=rss   19\n",
      "Invalid feed\n",
      "Generating features for  https://www.spiritz.in/feed/   20\n",
      "Generating features for  https://blog.cartoonmovement.com/rss.xml   21\n",
      "Generating features for  https://careandcomfortathome.com/feed/   22\n",
      "Invalid feed\n",
      "Generating features for  http://englishlassinla.com/feed/?x=1   23\n",
      "Could not parse feed  http://englishlassinla.com/feed/?x=1\n",
      "('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Generating features for  https://www.knowfengshui.com/feed/   24\n",
      "Invalid feed\n",
      "Generating features for  http://www.afriendtoknitwith.com/feeds/posts/default   25\n",
      "Invalid feed\n",
      "Generating features for  https://blog.pozible.com/feed   26\n",
      "Generating features for  https://dianic-boutique.com/feed   27\n",
      "Could not parse feed  https://dianic-boutique.com/feed\n",
      "HTTPSConnectionPool(host='dianic-boutique.com', port=443): Max retries exceeded with url: /feed (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000203D47C0CD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Generating features for  https://www.deeprooteddesigns.com/blog/feed/   28\n",
      "Invalid feed\n",
      "Generating features for  https://www.sfgirlbybay.com/category/inspiring-interiors/feed/   29\n",
      "Generating features for  https://www.somebodysavemusic.uk/blog?format=rss   30\n",
      "Invalid feed\n",
      "Generating features for  https://www.sallymatheny.com/feed/   31\n",
      "Generating features for  https://www.youtube.com/feeds/videos.xml?channel_id=UChiEokqXOZ3kFIISp0e5N4g   32\n",
      "Invalid feed\n",
      "Generating features for  https://blog.influenceandco.com/rss.xml   33\n",
      "Generating features for  https://www.youtube.com/feeds/videos.xml?user=JustAquascaping   34\n",
      "Invalid feed\n",
      "Generating features for  https://restaurant.eatapp.co/blog/rss.xml   35\n",
      "Generating features for  https://news.codecademy.com/rss/   36\n",
      "Generating features for  https://alltracksacademy.com/feed/   37\n",
      "Generating features for  https://gigibloks.com/blogs/news.atom   38\n",
      "Invalid feed\n",
      "Generating features for  https://bitemycoin.com/tag/litecoin/feed/   39\n",
      "Generating features for  https://www.garloward.com/blog/feed/   40\n",
      "Invalid feed\n",
      "Generating features for  https://pacificpetite.com/feed/   41\n",
      "Generating features for  https://www.optimum-coaching.co.uk/feed   42\n",
      "Invalid feed\n",
      "Generating features for  https://blog.epiccosplay.com/feed/   43\n",
      "Generating features for  https://www.ottawashowbox.com/feed/   44\n",
      "Generating features for  https://everydayrecycler.com/feed/   45\n",
      "Generating features for  https://tim.klingeleers.be/feed/   46\n",
      "Invalid feed\n",
      "Generating features for  http://frgcb.blogspot.com/feeds/posts/default?alt=rss   47\n",
      "Generating features for  http://nextgenseek.com/feed/   48\n",
      "Could not parse feed  http://nextgenseek.com/feed/\n",
      "HTTPConnectionPool(host='nextgenseek.com', port=80): Max retries exceeded with url: /feed/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000203D55374F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Generating features for  http://feeds.feedburner.com/MommaKatAndHerBearCat   49\n",
      "Generating features for  https://blog.kellywilliamsphotographer.com/category/portraits/feed/   50\n",
      "Invalid feed\n",
      "Generating features for  https://museumofthelost.com/feed   51\n",
      "Could not parse feed  https://museumofthelost.com/feed\n",
      "HTTPSConnectionPool(host='museumofthelost.com', port=443): Max retries exceeded with url: /feed (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000203D49A3190>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Generating features for  https://amwatmovers.com/feed/   52\n",
      "Generating features for  http://employmentblog.practicallaw.com/feed/   53\n",
      "Generating features for  https://jillsavage.org/feed/   54\n",
      "Generating features for  https://www.youtube.com/feeds/videos.xml?user=Jakkspr2   55\n",
      "Invalid feed\n",
      "Generating features for  https://wildeastfootball.net/feed/   56\n",
      "Invalid feed\n",
      "Generating features for  https://helpingthelittleguy.com/feed/   57\n",
      "Could not parse feed  https://helpingthelittleguy.com/feed/\n",
      "HTTPSConnectionPool(host='helpingthelittleguy.com', port=443): Max retries exceeded with url: /feed/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000203D45D4F40>, 'Connection to helpingthelittleguy.com timed out. (connect timeout=10)'))\n",
      "Generating features for  https://rhymeandribbons.com/feed   58\n",
      "Generating features for  https://www.easel.ly/blog/feed/   59\n",
      "Generating features for  https://www.blackhawkpartners.com/feed/   60\n",
      "Invalid feed\n",
      "Generating features for  https://myfoggybrain.com/feed/   61\n",
      "Generating features for  http://prsay.prsa.org/feed/   62\n",
      "Generating features for  http://feeds.feedburner.com/blogspot/NcVEB   63\n",
      "Invalid feed\n",
      "Generating features for  https://glassvodka.com/feed/   64\n",
      "Generating features for  https://www.recoveryhomeimprovement.com/blog/feed/   65\n",
      "Invalid feed\n",
      "Generating features for  https://stevenkozak.com/feed/   66\n",
      "Could not parse feed  https://stevenkozak.com/feed/\n",
      "('Connection aborted.', OSError(0, 'Error'))\n",
      "Generating features for  https://venturefoodtrucks.com/feed/   67\n",
      "Invalid feed\n",
      "Generating features for  https://www.thehikinglife.com/feed   68\n",
      "Invalid feed\n",
      "Generating features for  http://www.raindanceot.com/feed/   69\n",
      "Generating features for  https://realmomjobs.com/feed/   70\n",
      "Generating features for  https://legionathletics.com/category/building-muscle/feed/   71\n",
      "Generating features for  https://supportafterabortion.com/healing-from-abortion-blog/feed/   72\n",
      "Invalid feed\n",
      "Generating features for  http://www.fashionbysobczak.dk/feed   73\n",
      "Invalid feed\n",
      "Generating features for  https://www.youtube.com/feeds/videos.xml?channel_id=UCyqcJGTkMbzNr9WKr7Qw-aQ   74\n",
      "Invalid feed\n",
      "Generating features for  https://hk.asiatatler.com/feed   75\n",
      "Invalid feed\n",
      "Generating features for  https://www.newbloomsolutions.com/blog-feed.xml   76\n",
      "Generating features for  http://stirandstrain.com/feed/   77\n",
      "Generating features for  https://mcguffeymontessori.com/feed/   78\n",
      "Invalid feed\n",
      "Generating features for  https://kevinstaker.wordpress.com/feed/   79\n",
      "Generating features for  https://www.amodernmomblog.com/feed   80\n",
      "Invalid feed\n",
      "Generating features for  http://blog.joiningtech.com/rss.xml   81\n",
      "Invalid feed\n",
      "Generating features for  https://meanderwander.com/feed/   82\n",
      "Generating features for  https://endingchildpoverty.org/en/blog?format=feed&type=rss   83\n",
      "Invalid feed\n",
      "Generating features for  https://magehit.com/blog/feed   84\n",
      "Could not parse feed  https://magehit.com/blog/feed\n",
      "('Connection aborted.', OSError(0, 'Error'))\n",
      "Generating features for  https://huddlestontaxcpas.com/blog/feed/   85\n",
      "Invalid feed\n",
      "Generating features for  https://mutualfundsgrow.com/feed/   86\n",
      "Generating features for  https://www.styleatacertainage.com/category/fashion/feed/   87\n",
      "Generating features for  http://www.saltandserenity.com/category/pizza/feed/   88\n",
      "Generating features for  https://www.durhamnephrology.com/feed/   89\n",
      "Invalid feed\n",
      "Generating features for  http://blog.diversitynursing.com/blog/rss.xml   90\n",
      "Generating features for  https://blog.purse.io/feed   91\n",
      "Generating features for  https://www.williamburrows.com/   92\n",
      "Invalid feed\n",
      "Generating features for  https://travelingcanucks.com/feed/   93\n",
      "Generating features for  https://www.thetechieteacher.net/feeds/posts/default?alt=rss   94\n",
      "Generating features for  https://www.bangladeshibanker.com/feeds/posts/default   95\n",
      "Generating features for  https://ottawadance.wordpress.com/feed/   96\n",
      "Generating features for  https://contentsquare.com/insights/blog/feed/   97\n",
      "Invalid feed\n",
      "Generating features for  https://career.uconn.edu/blog/feed/   98\n",
      "Generating features for  http://feeds2.feedburner.com/CoverageCounsel   99\n",
      "Generating features for  https://www.india-briefing.com/feed/   100\n",
      "Invalid feed\n",
      "\n",
      "Features generated for 100 urls\n",
      "\n",
      "Saving features...\n",
      "Saved feed information\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate features for the feeds\n",
    "# specify for how many feeds to generate the features\n",
    "def generateFeatures(no_feeds):\n",
    "\n",
    "    loadFeedUrls(no_feeds)\n",
    "\n",
    "    # generate features for all feeds\n",
    "    for url in feed_url_list:   \n",
    "        parseFeed(url)\n",
    "\n",
    "    print('\\nFeatures generated for ' + str(count) + ' urls\\n')\n",
    "\n",
    "    # save feed information\n",
    "    print('Saving features...')\n",
    "\n",
    "    # connect to the db\n",
    "    conn = sqlite3.connect('feeds.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    for url, info in feed_info.items():\n",
    "        c.execute('UPDATE feeds SET text = ? WHERE url = ?;', (info['text'], url))\n",
    "        c.execute('UPDATE feeds SET title = ? WHERE url = ?;', (info['title'], url))\n",
    "        c.execute('UPDATE feeds SET description = ? WHERE url = ?;', (info['description'], url))\n",
    "\n",
    "    # commit and close connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    print('Saved feed information\\n')\n",
    "\n",
    "for i in range(2):\n",
    "    generateFeatures(100)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7eabe99ffc3d30b494d114e08fa0ce97bc43ac1aba15f2047c4ea25116d822e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
