{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Plot\n",
    "\n",
    "Use the bodies of text that describes a feed to create a UMAP Plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Features\n",
    "\n",
    "Load all information for all feeds or posts in the database that have features.\n",
    "\n",
    "Store the results in the *feeds* or *posts* dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import umap, umap.plot\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from corextopic import corextopic as ct\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import scipy\n",
    "\n",
    "# stop words lists\n",
    "from nltk.corpus import stopwords\n",
    "sw_nltk = stopwords.words('english')\n",
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# stores the posts in a dict\n",
    "feeds = dict()\n",
    "\n",
    "print('Loading feed info...')\n",
    "\n",
    "# connect to the db\n",
    "conn = sqlite3.connect('feeds.db')\n",
    "c = conn.cursor()\n",
    "    \n",
    "# select only the feeds for which the body of text \n",
    "# has already been generated\n",
    "c.execute('SELECT url, text, title, description FROM feeds WHERE text IS NOT NULL AND title IS NOT NULL AND description IS NOT NULL;')\n",
    "for entry in c.fetchall():\n",
    "    feeds[entry[0]] = {\n",
    "        'text': entry[1],\n",
    "        'title': entry[2],\n",
    "        'description': entry[3]\n",
    "    }        \n",
    "\n",
    "print('Loaded info for ' + str(len(feeds)) + ' feeds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(f['text'].split()) for f in feeds.values()]\n",
    "\n",
    "print('Minimum length of text:', min(lengths))\n",
    "print('Average length of text:', np.average(lengths))\n",
    "print('Maximum length of text:', max(lengths))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Pre-processor\n",
    "\n",
    "Define custom pre-processor for the vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will be used to pre-process data before being vectorized\n",
    "def custom_preprocessor(text):\n",
    "\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # lemmatize text using WordNet\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lemmatized_words = list()\n",
    "    for word in word_tokenize(text):\n",
    "\n",
    "        # determine the part-of-speech tag for the word\n",
    "        pos = nltk.pos_tag([word])[0][1][0] # only the first letter matters (hence the 0 at the end)\n",
    "\n",
    "        # default to noun if the tag does not fall into the categories accepted by lemmatize()\n",
    "        if pos != 'V' and pos != 'N' and pos != 'R' and pos != 'J':\n",
    "            pos = 'N'\n",
    "        if pos == 'J':\n",
    "            pos = 'A'\n",
    "        pos = pos.lower()\n",
    "\n",
    "        # lemmatize the word and add to list\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(word, pos))\n",
    "\n",
    "    # join the lemmatized words together\n",
    "    text = ' '.join(lemmatized_words)   \n",
    "\n",
    "    return text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Matrix\n",
    "\n",
    "Generate a doc-word matrix with binary counts for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(f['text'] for f in feeds.values())\n",
    "\n",
    "# generate a stop words list by joining multiple predefined lists\n",
    "stopwords_list = list(set(sw_nltk) | set(sw_spacy) | set(STOPWORDS) | set(ENGLISH_STOP_WORDS))\n",
    "\n",
    "# custom stop words (was given warning that these are not included in stop_words)\n",
    "stopwords_list.extend(['doe', 'ha', 'le', 'need', 'sha', 'wa', 'wo'])\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=20000, preprocessor=custom_preprocessor, binary=True, stop_words=stopwords_list)\n",
    "doc_word = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# get the words (column labels)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# get the docs/feeds (keys of the feeds dict)\n",
    "docs = list(feeds.keys())\n",
    "\n",
    "print('Shape of doc-word matrix: ', str(doc_word.shape))\n",
    "print('Number of documents (posts): ', str(len(docs)))\n",
    "print('Number of words (features): ', str(len(words)))\n",
    "\n",
    "# save the binary vectorized data\n",
    "scipy.sparse.save_npz('binary_matrix', doc_word)\n",
    "np.save('binary_words', np.asarray(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Matrix\n",
    "\n",
    "Generate a doc-word matrix with TF-IDF values for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfIdfVectorizer = TfidfVectorizer(max_features=20000, preprocessor=custom_preprocessor, stop_words=stopwords_list)\n",
    "doc_word_tfidf = tfIdfVectorizer.fit_transform(corpus)\n",
    "\n",
    "# get the words (column labels)\n",
    "words_tfidf = tfIdfVectorizer.get_feature_names()\n",
    "\n",
    "# save the binary vectorized data\n",
    "scipy.sparse.save_npz('tfidf_matrix', doc_word_tfidf)\n",
    "np.save('tfidf_words', np.asarray(words_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Matrices\n",
    "\n",
    "Load the vectorized data from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the docs/feeds (keys of the feeds dict)\n",
    "docs = list(feeds.keys())\n",
    "\n",
    "# binary matrix\n",
    "doc_word = scipy.sparse.load_npz('binary_matrix.npz')\n",
    "words =  np.load('binary_words.npy')\n",
    "\n",
    "# tfidf matrix\n",
    "doc_word_tfidf = scipy.sparse.load_npz('tfidf_matrix.npz')\n",
    "words_tfidf = np.load('tfidf_words.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_words = []\n",
    "for word in words:\n",
    "    if len(word) <= 3:\n",
    "        small_words.append(word)\n",
    "\n",
    "print(len(small_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CorEx Topic Modelling\n",
    "\n",
    "Use the CorEx library to infer topics from the bodies of text of the feeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the CorEx topic model\n",
    "topic_model = ct.Corex(n_hidden=25, words=words, docs=docs, max_iter=200, verbose=False, seed=1)\n",
    "topic_model.fit(doc_word, words=words, docs=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Distribution of TCs For Each Topic\n",
    "\n",
    "Use the plot to select an appropriate number of topics. Keep adding topics until additional ones do not significantly contribute to the overall TC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Correlation of the model:', str(topic_model.tc))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_model.tcs.shape[0]), topic_model.tcs, color='#4e79a7', width=0.5)\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display All Topics Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = topic_model.get_topics(n_words=10)\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ', '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corextopic.vis_topic as vt\n",
    "\n",
    "tm_layer2 = ct.Corex(n_hidden=25, docs=docs, max_iter=200, verbose=False, seed=1)\n",
    "tm_layer2.fit(topic_model.labels, docs=docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Topic and Top Feeds for Random Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the word in the vocabulary\n",
    "word_index = list(words).index('motherhood')\n",
    "\n",
    "# Get the topic associated with that word\n",
    "word_topic = topic_model.clusters[word_index]\n",
    "\n",
    "# Get top 10 documents for the topic\n",
    "top_docs = topic_model.get_top_docs(topic=word_topic, n_docs=15, sort_by='log_prob')\n",
    "\n",
    "print('Topic', word_topic)\n",
    "for doc in top_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_model.p_y_given_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "rgb = pca.fit_transform(topic_model.p_y_given_x)\n",
    "\n",
    "rgb = rgb - np.min(rgb, axis=0)\n",
    "rgb = rgb / np.max(rgb, axis=0)\n",
    "rgb = rgb * 255\n",
    "\n",
    "hex = []\n",
    "for rgb_color in rgb:\n",
    "    \n",
    "    r = int(rgb_color[0])\n",
    "    g = int(rgb_color[1])\n",
    "    b = int(rgb_color[2])\n",
    "\n",
    "    hex.append('#%02x%02x%02x' % (r, g, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign a Topic to Each Feed\n",
    "\n",
    "For each feed, assign it the topic such that the feed has the highest probability of belonging to that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a hard assignment of one topic per document\n",
    "hard_labels = np.zeros(doc_word.shape[0])\n",
    "\n",
    "for i in range(hard_labels.shape[0]):\n",
    "    hard_labels[i] = np.argmax(topic_model.p_y_given_x[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Probabilities\n",
    "\n",
    "Create an interactive UMAP plot based on the probabilities that each feed belongs to a topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit UMAP model\n",
    "mapper = umap.UMAP().fit(topic_model.p_y_given_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to be displayed when hovering over a point in the interactive plot\n",
    "hover_data = pd.DataFrame({\n",
    "    'title': [f['title'] for f in list(feeds.values())],\n",
    "    'description': [f['description'] for f in list(feeds.values())],\n",
    "    'label': hard_labels\n",
    "})\n",
    "\n",
    "umap.plot.output_notebook()\n",
    "p = umap.plot.interactive(mapper, labels=hard_labels, hover_data=hover_data, point_size=4)\n",
    "umap.plot.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Use TF-IDF values without topic modelling in the UMAP plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit umap with tf-idf doc word matrix (without topic modelling)\n",
    "tfidf_raw_mapper = umap.UMAP().fit(doc_word_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_data = pd.DataFrame({\n",
    "    'title': [f['title'] for f in feeds.values()],\n",
    "    'description': [f['description'] for f in feeds.values()],\n",
    "})\n",
    "\n",
    "labels = [f['title'] for f in feeds.values()]\n",
    "\n",
    "umap.plot.output_notebook()\n",
    "p = umap.plot.interactive(tfidf_raw_mapper, hover_data=hover_data, point_size=4)\n",
    "umap.plot.show(p)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7eabe99ffc3d30b494d114e08fa0ce97bc43ac1aba15f2047c4ea25116d822e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
