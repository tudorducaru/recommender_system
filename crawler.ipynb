{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "\n",
    "The script crawls articles on Feedspot's blog to gather RSS feeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Loads the following data from the database and stores it in global variables:\n",
    "* Feed urls\n",
    "* Unvisited lists (articles) urls\n",
    "* Visited lists (articles) urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# first list url to be scraped if there is not data in db\n",
    "firstListUrl = 'https://blog.feedspot.com/uk_rss_feeds/'\n",
    "\n",
    "# list of all feeds\n",
    "feeds = []\n",
    "\n",
    "unvisitedListUrls = []\n",
    "visitedListUrls = []\n",
    "\n",
    "# initial lengths of lists to make sure\n",
    "# only new data is added to the db\n",
    "initialNoFeeds = 0\n",
    "initialNoVisitedLists = 0\n",
    "\n",
    "# number n of visited lists\n",
    "# will be used to remove top n urls from db\n",
    "visitedLists = 0\n",
    "\n",
    "# load data from the database\n",
    "def loadData():\n",
    "\n",
    "    print('Loading data from db...')\n",
    "\n",
    "    # connect to the database\n",
    "    conn = sqlite3.connect('feeds_dev.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # load feeds\n",
    "    c.execute('SELECT url FROM feeds;')\n",
    "    global feeds\n",
    "    feeds = [i[0] for i in c.fetchall()]\n",
    "\n",
    "    global initialNoFeeds\n",
    "    initialNoFeeds = len(feeds)\n",
    "    print('Loaded ' + str(initialNoFeeds) + ' feeds')\n",
    "\n",
    "    # load unvisited lists\n",
    "    c.execute('SELECT url FROM unvisited_lists;')\n",
    "    global unvisitedListUrls\n",
    "    unvisitedListUrls = [i[0] for i in c.fetchall()]\n",
    "\n",
    "    global initialNoUnvisitedLists\n",
    "    initialNoUnvisitedLists = len(unvisitedListUrls)\n",
    "    print('Loaded ' + str(initialNoUnvisitedLists) + ' unvisited lists')\n",
    "    \n",
    "    # add an url if the db is empty\n",
    "    if (len(unvisitedListUrls) == 0):\n",
    "        unvisitedListUrls.append(firstListUrl)\n",
    "\n",
    "    # load visited lists\n",
    "    c.execute('SELECT url FROM visited_lists;')\n",
    "    global visitedListUrls\n",
    "    visitedListUrls = [i[0] for i in c.fetchall()]\n",
    "\n",
    "    global initialNoVisitedLists\n",
    "    initialNoVisitedLists = len(visitedListUrls)\n",
    "    print('Loaded ' + str(initialNoVisitedLists) + ' visited lists')\n",
    "    print('\\n')\n",
    "\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape List of Feeds\n",
    "\n",
    "Define a function that scrapes the url of a given Feedspot article that contains a list of RSS feeds, as well as a list of links to similar articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves all rss feed links from the given url\n",
    "# and adds them to the global variable\n",
    "def scrapeUrl(url):\n",
    "\n",
    "    # retrieve the webpage content\n",
    "    # include user-agent to ensure the response is not 403 Forbidden\n",
    "    try: \n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Mobile Safari/537.36'}\n",
    "        webpage_response = requests.get(url, headers=headers)\n",
    "\n",
    "        # get the rss feeds on that webpage\n",
    "        soup = BeautifulSoup(webpage_response.content, 'html.parser')\n",
    "        for tag in soup.select('.trow .fa-rss + a'):\n",
    "\n",
    "            href = tag.attrs['href']\n",
    "            if href != '' and href not in feeds:\n",
    "                feeds.append(tag.attrs['href'])\n",
    "\n",
    "        # get the links to the feed lists on the page\n",
    "        for tag in soup.select('.et_pb_extra_column_sidebar a'):\n",
    "\n",
    "            # href only contains the path, construct complete url\n",
    "            listUrl = 'https://blog.feedspot.com' + tag.attrs['href']\n",
    "            \n",
    "            # if the list has not been visited before,\n",
    "            # add it to the unvisited lists\n",
    "            \n",
    "            # there may be lists of other content types (blog, website etc.) \n",
    "            # only add rss feed lists\n",
    "            if listUrl not in visitedListUrls and listUrl not in unvisitedListUrls and 'rss_feeds' in listUrl:\n",
    "                unvisitedListUrls.append(listUrl)\n",
    "    except:\n",
    "        print('Could not scrape url: ' + url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl\n",
    "\n",
    "Define a function that scrapes the contents of the initial url provided and then subsequently scrapes each unvisited url (stored in a global list).\n",
    "\n",
    "The crawler stops when there are no more unvisited urls or when a predefined number of feeds (passed as an argument to the function) has been scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide a maximum number of feeds to be collected\n",
    "def crawl(maxNoOfFeeds):\n",
    "\n",
    "    # scrape webpages until there are no more lists to be scraped\n",
    "    # or the maximum number of feeds has been exceeded\n",
    "    while unvisitedListUrls and len(feeds) - initialNoFeeds < maxNoOfFeeds:\n",
    "\n",
    "        listUrl = unvisitedListUrls.pop(0)\n",
    "\n",
    "        # increment the number of visited lists in this run of the crawler\n",
    "        global visitedLists\n",
    "        visitedLists += 1\n",
    "\n",
    "        # add the list URL to the visited URLs\n",
    "        visitedListUrls.append(listUrl)\n",
    "\n",
    "        print('Scraping list: ', listUrl)\n",
    "        scrapeUrl(listUrl)\n",
    "        print('Number of feeds: ', len(feeds))\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Database\n",
    "\n",
    "Delete all tables from the database and recreate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the sqlite3 database of feeds\n",
    "def resetDatabase():\n",
    "    \n",
    "    # connect to the database\n",
    "    conn = sqlite3.connect('feeds_dev.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # drop the tables\n",
    "    print('Deleting tables...')\n",
    "    c.execute('DROP TABLE IF EXISTS feeds;')\n",
    "    c.execute('DROP TABLE IF EXISTS posts;')\n",
    "    c.execute('DROP TABLE IF EXISTS unvisited_lists;')\n",
    "    c.execute('DROP TABLE IF EXISTS visited_lists')\n",
    "\n",
    "    # re-create the tables\n",
    "    print('Creating tables... \\n')\n",
    "    c.execute('CREATE TABLE feeds (_id INTEGER PRIMARY KEY, url TEXT, text TEXT, title TEXT, description TEXT);')\n",
    "    c.execute('CREATE TABLE posts (_id INTEGER PRIMARY KEY, title TEXT, description TEXT, text TEXT, feed_title TEXT, FOREIGN KEY (feed_title) REFERENCES feeds (title);')\n",
    "    c.execute('CREATE TABLE unvisited_lists (_id INTEGER PRIMARY KEY, url TEXT);')\n",
    "    c.execute('CREATE TABLE visited_lists (_id INTEGER PRIMARY KEY, url TEXT);')    \n",
    "\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data\n",
    "\n",
    "Define a function that saves all data that has been gathered by the crawler in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves all feeds from the global list into the sqlite3 database\n",
    "def saveData():\n",
    "\n",
    "    print('Saving feeds...')\n",
    "\n",
    "    # connect to the database\n",
    "    conn = sqlite3.connect('feeds_dev.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # save feeds\n",
    "    counter = 0\n",
    "    for feed in feeds[initialNoFeeds:]:\n",
    "        c.execute('INSERT INTO feeds (url) VALUES (?);', (feed,))\n",
    "        counter += 1\n",
    "\n",
    "    # update the unvisited lists in the database\n",
    "    c.execute('DELETE FROM unvisited_lists;')\n",
    "    for l in unvisitedListUrls:\n",
    "        c.execute('INSERT INTO unvisited_lists (url) VALUES (?);', (l,))\n",
    " \n",
    "    # save visited lists\n",
    "    for l in visitedListUrls[initialNoVisitedLists:]:\n",
    "        c.execute('INSERT INTO visited_lists (url) VALUES (?);', (l, ))\n",
    "\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "    print('Saved ' + str(counter) + ' feeds')\n",
    "    conn.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from db...\n",
      "Loaded 13168 feeds\n",
      "Loaded 430 unvisited lists\n",
      "Loaded 311 visited lists\n",
      "\n",
      "\n",
      "Scraping list:  https://blog.feedspot.com/australian_beauty_rss_feeds/\n",
      "Number of feeds:  13241\n",
      "\n",
      "\n",
      "Scraping list:  https://blog.feedspot.com/afl_rss_feeds/\n",
      "Number of feeds:  13265\n",
      "\n",
      "\n",
      "Scraping list:  https://blog.feedspot.com/australian_photography_rss_feeds/\n",
      "Number of feeds:  13337\n",
      "\n",
      "\n",
      "Saving feeds...\n",
      "Saved 169 feeds\n"
     ]
    }
   ],
   "source": [
    "loadData()\n",
    "crawl(100)\n",
    "saveData()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7eabe99ffc3d30b494d114e08fa0ce97bc43ac1aba15f2047c4ea25116d822e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
