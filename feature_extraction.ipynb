{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "Script that extracts features from the feeds (given the urls in the database)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Feed URLs\n",
    "\n",
    "Define a function that loads the urls of the feeds that have not been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feed urls...\n",
      "Loaded 100 feeds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import feedparser\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "\n",
    "count = 0\n",
    "\n",
    "# list of feed urls loaded from the db\n",
    "feed_url_list = []\n",
    "\n",
    "# dict of feeds\n",
    "# urls are keys the values are dicts containing\n",
    "# features, title and description for that feed\n",
    "feeds = dict()\n",
    "\n",
    "# load feeds urls of feeds that do not have features yet\n",
    "def loadFeedUrls(no_feeds):\n",
    "\n",
    "    # clear feed urls list and feeds dict\n",
    "    feed_url_list.clear()\n",
    "    feeds.clear()\n",
    "\n",
    "    global count\n",
    "    count = 0\n",
    "\n",
    "    print('Loading feed urls...')\n",
    "\n",
    "    # connect to the db\n",
    "    conn = sqlite3.connect('feeds_dev.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # select urls \n",
    "    c.execute('SELECT url FROM feeds WHERE _id > (SELECT _id FROM feeds WHERE text IS NOT NULL AND title IS NOT NULL AND description IS NOT NULL ORDER BY _id DESC LIMIT 1) LIMIT ?;', (no_feeds,))    \n",
    "    for entry in c.fetchall():\n",
    "        feed_url_list.append(entry[0])\n",
    "\n",
    "    print('Loaded ' + str(len(feed_url_list)) + ' feeds\\n')\n",
    "\n",
    "loadFeedUrls(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Normalization Steps\n",
    "\n",
    "Define a function that normalizes a string through the following pipeline:\n",
    "* Remove HTML tags\n",
    "* Convert to lower case\n",
    "* Remove numbers\n",
    "* Remove symbols\n",
    "* Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizes a string\n",
    "def normalize(text):\n",
    "\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # remove symbols\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'[^A-Za-z ]+', '', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "\n",
    "    # remove stop words\n",
    "    tokenized = word_tokenize(text)\n",
    "    text = ' '.join([word for word in tokenized if not word in stopwords.words('english')])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse a Feed\n",
    "\n",
    "Define a function that parses a feed given its url as a parameter.\n",
    "\n",
    "### Phase 1\n",
    "\n",
    "Create a body of text from the following information:\n",
    "* Title of the feed\n",
    "* Description of the feed\n",
    "* Title of each entry\n",
    "* Description of each entry\n",
    "\n",
    "### Phase 2\n",
    "\n",
    "Normalize the generated body of text the using the function defined above. This will result in the body of text that will further be used by the CorEx Topic Model to infert the feed's topics.\n",
    "\n",
    "### Phase 3\n",
    "\n",
    "Save the following information in the *feeds* dict:\n",
    "* Text (normalized body of text)\n",
    "* Title of the feed\n",
    "* Description of the feed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze a feed and generate its initial body of text\n",
    "def parseFeed(url):\n",
    "\n",
    "    global count\n",
    "    count += 1\n",
    "    print('Generating features for ', url, ' ', str(count))\n",
    "\n",
    "    # return if the feed can not be parsed\n",
    "    try:\n",
    "\n",
    "        # get the rss feed content from the url\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Mobile Safari/537.36'}\n",
    "        webpage = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "        d = feedparser.parse(webpage.content)\n",
    "    except Exception as e:\n",
    "        print('Could not parse feed ', url)\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    # body of text representing the features\n",
    "    features = ''\n",
    "\n",
    "    # check that the feed has a title, description and at least one entry\n",
    "    title = d['feed'].get('title')\n",
    "    description = d['feed'].get('description')\n",
    "    entries = d['entries']\n",
    "\n",
    "    if not title or not description or len(entries) == 0:\n",
    "        \n",
    "        # feed is invalid\n",
    "        return\n",
    "\n",
    "    # feed is valid, continue feature extraction\n",
    "    # add title and description to body of text\n",
    "    features = title + ' ' + description\n",
    "\n",
    "    # add the title and description of each entry to the body of text\n",
    "    for entry in entries:\n",
    "        \n",
    "        entry_title = entry.get('title')\n",
    "        entry_title = entry_title if entry_title is not None else ''\n",
    "\n",
    "        entry_description = entry.get('description')\n",
    "        entry_description = entry_description if entry_description is not None else ''\n",
    "\n",
    "        features = features + ' ' + entry_title + ' ' + entry_description\n",
    "\n",
    "    # normalize the body of text\n",
    "    features = normalize(features)\n",
    "    \n",
    "    # add the features to the feed's dict entry\n",
    "    feeds[url] = {\n",
    "        'title': title,\n",
    "        'description': description,\n",
    "        'features': features\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Feed Info\n",
    " \n",
    "Define a function that saves the information in the *feeds* dict into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save feed information from the dict to the database\n",
    "def saveFeedInfo():\n",
    "\n",
    "    print('Saving features...')\n",
    "\n",
    "    # connect to the db\n",
    "    conn = sqlite3.connect('feeds_dev.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    for url, info in feeds.items():\n",
    "        c.execute('UPDATE feeds SET text = ? WHERE url = ?;', (info['features'], url))\n",
    "        c.execute('UPDATE feeds SET title = ? WHERE url = ?;', (info['title'], url))\n",
    "        c.execute('UPDATE feeds SET description = ? WHERE url = ?;', (info['description'], url))\n",
    "\n",
    "    # Commit and close connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    print('Saved feed information\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Features\n",
    "\n",
    "Load and generate features for a predefined number of feeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate features for the feeds\n",
    "# specify for how many feeds to generate the features\n",
    "def generateFeatures(no_feeds):\n",
    "\n",
    "    loadFeedUrls(no_feeds)\n",
    "\n",
    "    # generate features for all feeds\n",
    "    for url in feed_url_list:   \n",
    "        parseFeed(url)\n",
    "\n",
    "    print('\\nFeatures generated for ' + str(count) + ' urls\\n')\n",
    "\n",
    "    # save the features\n",
    "    saveFeedInfo()\n",
    "\n",
    "# for i in range(5):\n",
    "#     generateFeatures(10)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7eabe99ffc3d30b494d114e08fa0ce97bc43ac1aba15f2047c4ea25116d822e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
